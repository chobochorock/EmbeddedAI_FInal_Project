import cv2
import numpy as np
import onnxruntime as ort
import sys
from PIL import Image, ImageDraw, ImageFont

# ì„¤ì •
ONNX_MODEL_PATH = "best.onnx"
LABEL_PATH = "labels.txt"
INPUT_WIDTH = 128
INPUT_HEIGHT = 128
CONFIDENCE_THRESHOLD = 0.4
FONT_PATH = "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc"

def load_classes(path):
    with open(path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f.readlines()]

def put_text_hanja(img, text, position, font_path, font_size, color):
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    try:
        font = ImageFont.truetype(font_path, font_size)
    except:
        # í°íŠ¸ íŒŒì¼ ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸(í•œì ì•ˆë‚˜ì˜´) ì‚¬ìš©
        font = ImageFont.load_default()
    
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

def main():
    classes = load_classes(LABEL_PATH)
    print(f"ì´ {len(classes)}ê°œì˜ í´ë˜ìŠ¤ ë¡œë“œ")

    # [í•µì‹¬ ë³€ê²½] OpenCV DNN ëŒ€ì‹  ONNX Runtime ì‚¬ìš©
    print("ONNX Runtimeìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # GPU(CUDA) ì‚¬ìš© ì„¤ì •
    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
    try:
        session = ort.InferenceSession(ONNX_MODEL_PATH, providers=providers)
        print("ONNX Runtime ë¡œë“œ ì„±ê³µ! (GPU ê°€ì† ì‹œë„)")
    except Exception as e:
        print(f"ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    # ì…ë ¥/ì¶œë ¥ ì´ë¦„ ì•Œì•„ë‚´ê¸°
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    # cap = cv2.VideoCapture(0) # í˜¹ì€ GStreamer ë¬¸ìì—´
    # 1. GStreamer íŒŒì´í”„ë¼ì¸ ë¬¸ìì—´ ìƒì„± í•¨ìˆ˜
    def gstreamer_pipeline(
        sensor_id=0,
        capture_width=1280,
        capture_height=720,
        display_width=128,
        display_height=128,
        framerate=30,
        flip_method=0,
    ):
        return (
            "nvarguscamerasrc sensor-id=%d ! "
            "video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, format=(string)NV12, framerate=(fraction)%d/1 ! "
            "nvvidconv flip-method=%d ! "
            "video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! "
            "videoconvert ! "
            "video/x-raw, format=(string)BGR ! appsink"
            % (
                sensor_id,
                capture_width,
                capture_height,
                framerate,
                flip_method,
                display_width,
                display_height,
            )
        )

    # 2. ì¹´ë©”ë¼ ì—´ê¸° (GStreamer ëª¨ë“œ ì‚¬ìš©)
    print("ğŸ“¸ CSI ì¹´ë©”ë¼ë¥¼ GStreamerë¡œ ì—¬ëŠ” ì¤‘...")
    cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)

    if cap.isOpened():
        print("âœ… ì¹´ë©”ë¼ ì—°ê²° ì„±ê³µ!")
    else:
        print("âŒ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: ë°ëª¬ì„ ì¬ì‹œì‘í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        sys.exit()
    
    # if not cap.isOpened():
    #     print("ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨")
    #     sys.exit()

    while True:
        ret, frame = cap.read()
        if not ret: break

        # ì „ì²˜ë¦¬ (OpenCV DNNê³¼ ì•½ê°„ ë‹¤ë¦„)
        # BGR -> RGB ë³€í™˜
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        # Resize
        img = cv2.resize(img, (INPUT_WIDTH, INPUT_HEIGHT))
        # 0~255 -> 0.0~1.0
        img = img.astype(np.float32) / 255.0 # float32
        # (H, W, C) -> (C, H, W) -> (1, C, H, W)
        img = img.transpose(2, 0, 1)
        blob = np.expand_dims(img, axis=0)

        # ì¶”ë¡  ì‹¤í–‰ (ONNX Runtime)
        outputs = session.run([output_name], {input_name: blob})[0]

        # í›„ì²˜ë¦¬ (ì´í›„ ë¡œì§ì€ ê¸°ì¡´ê³¼ ìœ ì‚¬í•˜ë‚˜ ë°ì´í„° í˜•íƒœì— ë”°ë¼ ì¡°ì • í•„ìš”)
        # YOLOv8 Output: (1, 4+cls, 8400)
        outputs = np.array([cv2.transpose(outputs[0])])
        rows = outputs.shape[1]

        boxes = []
        scores = []
        class_ids = []

        x_factor = frame.shape[1] / INPUT_WIDTH
        y_factor = frame.shape[0] / INPUT_HEIGHT

        for i in range(rows):
            classes_scores = outputs[0][i][4:]
            _, max_score, _, max_class_loc = cv2.minMaxLoc(classes_scores)
            
            if max_score >= CONFIDENCE_THRESHOLD:
                # ì¢Œí‘œ ê³„ì‚° ë“± ê¸°ì¡´ê³¼ ë™ì¼...
                class_id = max_class_loc[1]
                box = outputs[0][i][:4]
                x, y, w, h = box[0], box[1], box[2], box[3]
                
                left = int((x - 0.5 * w) * x_factor)
                top = int((y - 0.5 * h) * y_factor)
                width = int(w * x_factor)
                height = int(h * y_factor)
                
                boxes.append([left, top, width, height])
                scores.append(float(max_score))
                class_ids.append(class_id)

        # NMS ë° ê·¸ë¦¬ê¸° (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        indices = cv2.dnn.NMSBoxes(boxes, scores, 0.4, 0.45)
        
        if len(indices) > 0:
            for i in indices:
                idx = i if isinstance(i, int) else i[0]
                box = boxes[idx]
                left, top, width, height = box[0], box[1], box[2], box[3]
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸ ìƒì„±
                if class_ids[idx] < len(classes):
                    label_text = f"{classes[class_ids[idx]]}" # ì ìˆ˜ ë¹¼ê³  ê¸€ìë§Œ í¬ê²Œ
                else:
                    label_text = "Unknown"

                # 1. ë°•ìŠ¤ ê·¸ë¦¬ê¸° (OpenCV ì‚¬ìš©)
                cv2.rectangle(frame, (left, top), (left+width, top+height), (0, 255, 0), 2)
                
                # 2. [í•µì‹¬ ë³€ê²½] í•œì ê·¸ë¦¬ê¸° (PIL ì‚¬ìš©)
                # ê¸°ì¡´ cv2.putText(...) ì¤„ì„ ì§€ìš°ê³  ì•„ë˜ ì¤„ë¡œ êµì²´í•˜ì„¸ìš”.
                frame = put_text_hanja(frame, label_text, (left, top + 30), FONT_PATH, 30, (0, 255, 0))

        cv2.imshow("Hanja Detector", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
            
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()