import cv2
import numpy as np
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import sys
import time
from PIL import Image, ImageDraw, ImageFont

# ==========================================
# [ì„¤ì •]
ENGINE_PATH = "hanja_yolo3/best6_light.engine"
LABEL_PATH  = "hanja_yolo3/classes.txt"
# ìš°ë¶„íˆ¬ ê¸°ë³¸ í•œê¸€/í•œì í°íŠ¸ ê²½ë¡œ ì˜ˆì‹œ (ë‚˜ëˆ”ê³ ë”• ë“± ì¶”ì²œ)
# ë§Œì•½ íŒŒì¼ì´ ì—†ë‹¤ë©´ 'NanumGothic.ttf' ë“±ì„ í”„ë¡œì íŠ¸ í´ë”ì— ë„£ê³  ê²½ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
FONT_PATH   = "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc" 
INPUT_SIZE  = 640
CONF_THRESH = 0.4
IOU_THRESH  = 0.45
# ==========================================

TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

class TRTWrapper:
    def __init__(self, engine_path):
        try:
            with open(engine_path, "rb") as f, trt.Runtime(TRT_LOGGER) as runtime:
                self.engine = runtime.deserialize_cuda_engine(f.read())
            self.context = self.engine.create_execution_context()
        except FileNotFoundError:
            sys.exit(f"âŒ ì—”ì§„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {engine_path}")

        self.inputs, self.outputs, self.bindings, self.stream = [], [], [], cuda.Stream()
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding)) * self.engine.max_batch_size
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            self.bindings.append(int(device_mem))
            if self.engine.binding_is_input(binding):
                self.inputs.append({'host': host_mem, 'device': device_mem})
            else:
                self.outputs.append({'host': host_mem, 'device': device_mem})

    def infer(self, img):
        np.copyto(self.inputs[0]['host'], img.ravel())
        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)
        self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)
        for out in self.outputs:
            cuda.memcpy_dtoh_async(out['host'], out['device'], self.stream)
        self.stream.synchronize()
        return [out['host'] for out in self.outputs]

def gstreamer_pipeline(sensor_id=0, capture_width=1280, capture_height=720, display_width=640, display_height=640, framerate=30, flip_method=0):
    return (
        "nvarguscamerasrc sensor-id=%d ! "
        "video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, format=(string)NV12, framerate=(fraction)%d/1 ! "
        "nvvidconv flip-method=%d ! "
        "video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! "
        "videoconvert ! "
        "video/x-raw, format=(string)BGR ! appsink"
        % (sensor_id, capture_width, capture_height, framerate, flip_method, display_width, display_height)
    )

def main():
    # 1. í´ë˜ìŠ¤ ë¡œë“œ
    try:
        with open(LABEL_PATH, "r", encoding="utf-8") as f:
            classes = [line.strip() for line in f.readlines()]
    except FileNotFoundError:
        sys.exit(f"âŒ ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LABEL_PATH}")

    # 2. í°íŠ¸ ë¡œë“œ
    try:
        font = ImageFont.truetype(FONT_PATH, 20)
    except OSError:
        print(f"âš ï¸ ê²½ê³ : {FONT_PATH}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (í•œì ê¹¨ì§ ê°€ëŠ¥).")
        font = ImageFont.load_default()

    print("ğŸš€ TensorRT ì—”ì§„ ë¡œë”© ì¤‘...")
    trt_model = TRTWrapper(ENGINE_PATH)
    print("âœ… ë¡œë”© ì™„ë£Œ! ì¹´ë©”ë¼ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    # GStreamer íŒŒì´í”„ë¼ì¸ (ë””ìŠ¤í”Œë ˆì´ í¬ê¸°ì™€ ì…ë ¥ í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒì„ ëŒ€ë¹„)
    cap = cv2.VideoCapture(gstreamer_pipeline(display_width=INPUT_SIZE, display_height=INPUT_SIZE), cv2.CAP_GSTREAMER)
    
    if not cap.isOpened():
        sys.exit("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # FPS ê³„ì‚°ìš© ë³€ìˆ˜
    prev_time = 0

    while True:
        ret, frame = cap.read()
        if not ret: break

        h, w, _ = frame.shape
        
        # [ì „ì²˜ë¦¬] ë¹„ìœ¨ ìœ ì§€ë¥¼ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ íŒ©í„° ê³„ì‚° (í˜„ì¬ëŠ” 1:1ì´ì§€ë§Œ í™•ì¥ì„± ê³ ë ¤)
        scale_x = w / INPUT_SIZE
        scale_y = h / INPUT_SIZE

        img = cv2.resize(frame, (INPUT_SIZE, INPUT_SIZE))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.transpose((2, 0, 1)).astype(np.float32) / 255.0
        img = np.expand_dims(img, axis=0)

        # [ì¶”ë¡ ]
        output = trt_model.infer(img)[0]
        
        # [í›„ì²˜ë¦¬] YOLO ì¶œë ¥ í˜•ìƒì— ë§ê²Œ reshape (ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ, í™•ì¸ í•„ìš”)
        # ë³´í†µ YOLOv5/v8 export ì‹œ: [Batch, Anchors, 5+Classes] or [Batch, 5+Classes, Anchors]
        # ì—¬ê¸°ì„œëŠ” ì‘ì„±ìë¶„ì˜ ì½”ë“œ(flat -> reshape)ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
        output = output.reshape(1, -1, 5 + len(classes))
        output = output[0]

        boxes, scores, class_ids = [], [], []
        
        # ì‹ ë¢°ë„ í•„í„°ë§
        conf_mask = output[:, 4] > CONF_THRESH
        detections = output[conf_mask]

        for det in detections:
            confidence = det[4]
            class_probs = det[5:]
            class_id = np.argmax(class_probs)
            final_score = confidence * class_probs[class_id]

            if final_score > CONF_THRESH:
                # ì¢Œí‘œ ë³µì› (0~1 ì •ê·œí™”ëœ ê°’ì´ ì•„ë‹ˆë¼ í”½ì…€ ê°’ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤ê³  ê°€ì • - YOLO ë²„ì „ì— ë”°ë¼ ë‹¤ë¦„)
                # ë§Œì•½ ë°•ìŠ¤ê°€ ì´ìƒí•˜ê²Œ í¬ë‹¤ë©´ ì•„ë˜ ë¡œì§ í™•ì¸ í•„ìš”
                cx, cy, bw, bh = det[0:4]
                
                # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ì¶° ìŠ¤ì¼€ì¼ë§
                left = int((cx - 0.5 * bw) * scale_x)
                top = int((cy - 0.5 * bh) * scale_y)
                width = int(bw * scale_x)
                height = int(bh * scale_y)

                boxes.append([left, top, width, height])
                scores.append(float(final_score))
                class_ids.append(class_id)

        indices = cv2.dnn.NMSBoxes(boxes, scores, CONF_THRESH, IOU_THRESH)

        # [ê·¸ë¦¬ê¸° ë‹¨ê³„] ë°•ìŠ¤ê°€ ìˆì„ ë•Œë§Œ PIL ë³€í™˜ ìˆ˜í–‰ (ì†ë„ ìµœì í™”)
        if len(indices) > 0:
            img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(img_pil)

            for i in indices:
                idx = i if isinstance(i, int) else i[0]
                box = boxes[idx]
                left, top, width, height = box
                
                # ì¢Œí‘œê°€ ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šê²Œ í´ë¨í•‘
                left = max(0, left)
                top = max(0, top)

                # ë¼ë²¨ ìƒì„±
                label_text = f"{classes[class_ids[idx]]} {scores[idx]:.0%}"
                
                # í…ìŠ¤íŠ¸ ì‚¬ì´ì¦ˆ ê³„ì‚° (ë°°ê²½ ë°•ìŠ¤ í¬ê¸° ìë™ ì¡°ì ˆ)
                text_bbox = draw.textbbox((0, 0), label_text, font=font)
                text_w = text_bbox[2] - text_bbox[0]
                text_h = text_bbox[3] - text_bbox[1]

                # ë°•ìŠ¤ ë° í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
                draw.rectangle([left, top, left + width, top + height], outline=(0, 255, 0), width=3)
                draw.rectangle([left, top - text_h - 10, left + text_w + 10, top], fill=(0, 255, 0))
                draw.text((left + 5, top - text_h - 5), label_text, font=font, fill=(255, 255, 255))

            # ë‹¤ì‹œ OpenCV í¬ë§·ìœ¼ë¡œ
            frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

        # FPS ì¶œë ¥
        curr_time = time.time()
        fps = 1 / (curr_time - prev_time) if prev_time != 0 else 0
        prev_time = curr_time
        cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        cv2.imshow("Hanja TensorRT", frame)
        if cv2.waitKey(1) == ord('q'): break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()