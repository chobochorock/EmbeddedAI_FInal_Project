import cv2
import numpy as np
import onnxruntime as ort
import sys
from PIL import Image, ImageDraw, ImageFont

# ==========================================
# [ì„¤ì •] ë³¸ì¸ í™˜ê²½ì— ë§žê²Œ ìˆ˜ì •í•˜ì„¸ìš”
# ==========================================
ONNX_MODEL_PATH = "hanja_yolo3/best_fp16.onnx"   # PCì—ì„œ ê°€ì ¸ì˜¨ ONNX íŒŒì¼ (opset 10 ê¶Œìž¥)
LABEL_PATH      = "./hanja_yolo3/classes.txt" # í´ëž˜ìŠ¤ ì´ë¦„ì´ ì ížŒ íŒŒì¼
INPUT_SIZE      = 640           # í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í¬ê¸° (640 or 1280 ë“±)
CONF_THRESH     = 0.4           # íƒì§€ ì‹ ë¢°ë„ ê¸°ì¤€
FONT_PATH       = "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc" # í•œìž í°íŠ¸ ê²½ë¡œ
# ==========================================

def gstreamer_pipeline(sensor_id=0, capture_width=1280, capture_height=720, display_width=640, display_height=640, framerate=30, flip_method=0):
    """
    í•œìž ì¸ì‹ì„ ìœ„í•´ ì¤‘ì•™ ë¶€ë¶„ì„ í™•ëŒ€(Crop & Zoom)í•´ì„œ ê°€ì ¸ì˜¤ëŠ” GStreamer íŒŒì´í”„ë¼ì¸
    """
    crop_left, crop_right = 320, 1280-320
    crop_top, crop_bottom = 40, 720-40
    return (
        "nvarguscamerasrc sensor-id=%d ! "
        "video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, format=(string)NV12, framerate=(fraction)%d/1 ! "
        "nvvidconv left=%d right=%d top=%d bottom=%d flip-method=%d ! "
        "video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! "
        "videoconvert ! "
        "video/x-raw, format=(string)BGR ! appsink"
        % (sensor_id, capture_width, capture_height, framerate, crop_left, crop_right, crop_top, crop_bottom, flip_method, display_width, display_height)
    )

def main():
    # 1. í´ëž˜ìŠ¤ ë¡œë“œ
    try:
        with open(LABEL_PATH, "r", encoding="utf-8") as f:
            classes = [line.strip() for line in f.readlines()]
    except:
        print("âš ï¸ classes.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¼ë²¨ í‘œì‹œê°€ ì•ˆ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        classes = []

    # 2. í°íŠ¸ ë¡œë“œ
    try:
        font = ImageFont.truetype(FONT_PATH, 30)
    except:
        font = ImageFont.load_default()

    # 3. ONNX ëª¨ë¸ ë¡œë“œ (GPU ê°€ì† í™œì„±í™”)
    print(f"ðŸš€ ONNX ëª¨ë¸ ë¡œë”© ì¤‘: {ONNX_MODEL_PATH}")
    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
    try:
        session = ort.InferenceSession(ONNX_MODEL_PATH, providers=providers)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # 4. ì¹´ë©”ë¼ ì—°ê²°
    cap = cv2.VideoCapture(gstreamer_pipeline(), cv2.CAP_GSTREAMER)
    if not cap.isOpened():
        sys.exit("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("ðŸŽ¥ ì‹¤í–‰ ì‹œìž‘! (ì¢…ë£Œ: q)")

    while True:
        ret, frame = cap.read()
        if not ret: break

        # ------------------------------------------------
        # [ì „ì²˜ë¦¬] YOLOv5 ìž…ë ¥ í˜•ì‹ì— ë§žì¶”ê¸°
        # ------------------------------------------------
        # 1. Resize
        img = cv2.resize(frame, (INPUT_SIZE, INPUT_SIZE))
        # 2. BGR -> RGB
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # 3. Normalize (0~1) & Transpose (HWC -> CHW)
        img = img.transpose((2, 0, 1)).astype(np.float32) / 255.0
        # 4. Batch ì°¨ì› ì¶”ê°€ (1, 3, 640, 640)
        blob = np.expand_dims(img, axis=0)

        # ------------------------------------------------
        # [ì¶”ë¡ ] ONNX Runtime ì‹¤í–‰
        # ------------------------------------------------
        outputs = session.run([output_name], {input_name: blob})[0]

        # ------------------------------------------------
        # [í›„ì²˜ë¦¬] ê²°ê³¼ íŒŒì‹± (YOLOv5 Output)
        # ------------------------------------------------
        # Output shape: (1, 25200, 5+Class) -> (1, N, 85 ë“±)
        predictions = outputs[0] 

        boxes = []
        scores = []
        class_ids = []

        # ì›ë³¸ í•´ìƒë„ ë³µì›ì„ ìœ„í•œ ë¹„ìœ¨
        x_factor = frame.shape[1] / INPUT_SIZE
        y_factor = frame.shape[0] / INPUT_SIZE

        # ì‹ ë¢°ë„ í•„í„°ë§ (forë¬¸ ëŒ€ì‹  Numpy ì—°ì‚°ìœ¼ë¡œ ì†ë„ ìµœì í™”)
        # confidence(obj_conf) * class_score ê°€ ê¸°ì¤€ ì´ìƒì¸ ê²ƒë§Œ í•„í„°ë§
        
        # 4ë²ˆ ì¸ë±ìŠ¤(Objectness)ê°€ ìž„ê³„ê°’ë³´ë‹¤ í° ê²ƒë§Œ 1ì°¨ í•„í„°ë§
        conf_mask = predictions[:, 4] > CONF_THRESH
        detections = predictions[conf_mask]

        for det in detections:
            confidence = det[4]
            class_probs = det[5:]
            class_id = np.argmax(class_probs)
            class_score = class_probs[class_id]
            
            # ìµœì¢… ì ìˆ˜
            final_score = confidence * class_score
            
            if final_score > CONF_THRESH:
                x, y, w, h = det[0:4]
                
                # ì¢Œí‘œ ë³µì› (Center_XYWH -> TopLeft_XYWH)
                left = int((x - 0.5 * w) * x_factor)
                top = int((y - 0.5 * h) * y_factor)
                width = int(w * x_factor)
                height = int(h * y_factor)

                boxes.append([left, top, width, height])
                scores.append(float(final_score))
                class_ids.append(int(class_id))

        # NMS (ê²¹ì¹œ ë°•ìŠ¤ ì œê±°)
        indices = cv2.dnn.NMSBoxes(boxes, scores, CONF_THRESH, 0.45)

        # ----------------